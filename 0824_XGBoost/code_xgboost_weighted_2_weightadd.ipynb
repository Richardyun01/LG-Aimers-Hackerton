{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1WanDnyTocjV",
        "outputId": "091bdd92-904a-4603-ef62-c6f53369d928"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n    TODO\\n    1. 명칭 및 구조 통일화\\n    2. 그래프 및 그림 추가\\n    3. 피처 추가\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "    TODO\n",
        "    1. 명칭 및 구조 통일화\n",
        "    2. 그래프 및 그림 추가\n",
        "    3. 피처 추가\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9DeOvSWocjW",
        "outputId": "0de6bb82-3aa3-4dea-e5a2-77e9d09da5e8"
      },
      "outputs": [],
      "source": [
        "# pip install optuna\n",
        "# !pip install optuna-integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMmmTVC_o_Te",
        "outputId": "9be6cb23-9e47-41a7-c7e7-bbd777db3ce2"
      },
      "outputs": [],
      "source": [
        "# pip install torch pandas numpy scikit-learn xgboost optuna matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip uninstall torch torchvision torchaudio\n",
        "# !pip cache purge\n",
        "# !pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JKvnb7UfocjW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\syyun\\Desktop\\Programming\\Hackerton\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import xgboost as xgb\n",
        "from xgboost.callback import EvaluationMonitor\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D50WVxZAocjW",
        "outputId": "a864b1de-7193-4e5e-de13-6437fce02cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    CUDA = torch.cuda.is_available()\n",
        "except:\n",
        "    CUDA = False\n",
        "\n",
        "print(f\"CUDA available: {CUDA}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SunrOHBlocjW",
        "outputId": "5bb83b64-f765-43b8-e149-c7500addfa32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File paths are valid.\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# 1) 경로 설정\n",
        "# -----------------------------\n",
        "TRAIN_FP = Path('../open/train/train.csv')\n",
        "TEST_DIR = Path('../open/test')\n",
        "SAMPLE_FP = Path('../open/sample_submission.csv')\n",
        "\n",
        "if not TRAIN_FP.exists() or not TEST_DIR.exists() or not SAMPLE_FP.exists():\n",
        "    print(\"Path error: directory or file does not exist. Please check the paths.\")\n",
        "    print(f\"Expected file path: {TRAIN_FP}\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"File paths are valid.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWguEkjUocjX",
        "outputId": "afb4b3e5-99d8-429c-e9b0-508ca8ee6c74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loading and preprocessing...\n",
            "Outlier handling using IQR...\n",
            "Outlier handling complete\n",
            "[TEST_00] shape: (5404, 3) | 날짜: 2024-06-16 ~ 2024-07-13\n",
            "[TEST_01] shape: (5404, 3) | 날짜: 2024-07-21 ~ 2024-08-17\n",
            "[TEST_02] shape: (5404, 3) | 날짜: 2024-08-25 ~ 2024-09-21\n",
            "[TEST_03] shape: (5404, 3) | 날짜: 2024-09-29 ~ 2024-10-26\n",
            "[TEST_04] shape: (5404, 3) | 날짜: 2024-11-03 ~ 2024-11-30\n",
            "[TEST_05] shape: (5404, 3) | 날짜: 2024-12-08 ~ 2025-01-04\n",
            "[TEST_06] shape: (5404, 3) | 날짜: 2025-01-12 ~ 2025-02-08\n",
            "[TEST_07] shape: (5404, 3) | 날짜: 2025-02-16 ~ 2025-03-15\n",
            "[TEST_08] shape: (5404, 3) | 날짜: 2025-03-23 ~ 2025-04-19\n",
            "[TEST_09] shape: (5404, 3) | 날짜: 2025-04-27 ~ 2025-05-24\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# 2) 데이터 로드 및 전처리\n",
        "# -----------------------------\n",
        "print(\"Data loading and preprocessing...\")\n",
        "\n",
        "train = pd.read_csv('../open/train/train.csv')\n",
        "train[\"영업일자\"] = pd.to_datetime(train[\"영업일자\"])\n",
        "\n",
        "print(\"Outlier handling using IQR...\")\n",
        "\n",
        "# 분포 비교를 위해 사본 보관\n",
        "train_raw = train.copy()\n",
        "\n",
        "# 이상치 처리 함수\n",
        "def handle_outliers_iqr(df_group):\n",
        "    non_zero_sales = df_group[df_group[\"매출수량\"] > 0][\"매출수량\"]\n",
        "    if len(non_zero_sales) < 5:\n",
        "        return df_group\n",
        "\n",
        "    q1, q3 = non_zero_sales.quantile(0.25), non_zero_sales.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    # 이상치 기준\n",
        "    lower_bound = max(0, q1 - 1.5 * iqr)\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "    df_group[\"매출수량\"] = np.clip(df_group[\"매출수량\"], lower_bound, upper_bound)\n",
        "    return df_group\n",
        "\n",
        "train = train.groupby(\"영업장명_메뉴명\", group_keys=False).apply(handle_outliers_iqr)\n",
        "\n",
        "print(\"Outlier handling complete\")\n",
        "\n",
        "tests = {}\n",
        "for i in range(10):\n",
        "    name = f\"TEST_{i:02d}\"\n",
        "    df = pd.read_csv(TEST_DIR / f\"{name}.csv\")\n",
        "    df[\"영업일자\"] = pd.to_datetime(df[\"영업일자\"])\n",
        "    tests[name] = df\n",
        "    print(f\"[{name}] shape: {df.shape} | 날짜: {df['영업일자'].min().date()} ~ {df['영업일자'].max().date()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sYnntRLqhIl",
        "outputId": "24f2f70e-e6fd-467b-a0b4-43e65968f360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loading and preprocessing...\n",
            "Outlier handling using IQR...\n",
            "✅ Outlier handling complete\n",
            "[TEST_00] shape: (5404, 3) | 날짜: 2024-06-16 ~ 2024-07-13\n",
            "[TEST_01] shape: (5404, 3) | 날짜: 2024-07-21 ~ 2024-08-17\n",
            "[TEST_02] shape: (5404, 3) | 날짜: 2024-08-25 ~ 2024-09-21\n",
            "[TEST_03] shape: (5404, 3) | 날짜: 2024-09-29 ~ 2024-10-26\n",
            "[TEST_04] shape: (5404, 3) | 날짜: 2024-11-03 ~ 2024-11-30\n",
            "[TEST_05] shape: (5404, 3) | 날짜: 2024-12-08 ~ 2025-01-04\n",
            "[TEST_06] shape: (5404, 3) | 날짜: 2025-01-12 ~ 2025-02-08\n",
            "[TEST_07] shape: (5404, 3) | 날짜: 2025-02-16 ~ 2025-03-15\n",
            "[TEST_08] shape: (5404, 3) | 날짜: 2025-03-23 ~ 2025-04-19\n",
            "[TEST_09] shape: (5404, 3) | 날짜: 2025-04-27 ~ 2025-05-24\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# 라이브러리 임포트\n",
        "# -----------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# -----------------------------\n",
        "# 1) 경로 설정\n",
        "# -----------------------------\n",
        "TRAIN_FP = Path('../open/train/train.csv')\n",
        "TEST_DIR = Path('../open/test')\n",
        "SAMPLE_FP = Path('../open/sample_submission.csv')\n",
        "\n",
        "# -----------------------------\n",
        "# 2) 데이터 로드 및 전처리\n",
        "# -----------------------------\n",
        "print(\"Data loading and preprocessing...\")\n",
        "\n",
        "train = pd.read_csv(TRAIN_FP)\n",
        "train[\"영업일자\"] = pd.to_datetime(train[\"영업일자\"])\n",
        "\n",
        "print(\"Outlier handling using IQR...\")\n",
        "\n",
        "train_raw = train.copy()  # 분포 비교용 원본 저장\n",
        "\n",
        "# 이상치 처리 함수\n",
        "def handle_outliers_iqr(df_group):\n",
        "    non_zero_sales = df_group[df_group[\"매출수량\"] > 0][\"매출수량\"]\n",
        "    if len(non_zero_sales) < 5:\n",
        "        return df_group\n",
        "\n",
        "    q1, q3 = non_zero_sales.quantile(0.25), non_zero_sales.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    lower_bound = max(0, q1 - 1.5 * iqr)\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "    df_group.loc[:, \"매출수량\"] = np.clip(df_group[\"매출수량\"], lower_bound, upper_bound)\n",
        "    return df_group\n",
        "\n",
        "# 그룹별 이상치 처리\n",
        "train = train.groupby(\"영업장명_메뉴명\", group_keys=False).apply(handle_outliers_iqr)\n",
        "\n",
        "print(\"✅ Outlier handling complete\")\n",
        "\n",
        "# Test 데이터 로드\n",
        "tests = {}\n",
        "for i in range(10):\n",
        "    name = f\"TEST_{i:02d}\"\n",
        "    df = pd.read_csv(TEST_DIR / f\"{name}.csv\")\n",
        "    df[\"영업일자\"] = pd.to_datetime(df[\"영업일자\"])\n",
        "    tests[name] = df\n",
        "    print(f\"[{name}] shape: {df.shape} | 날짜: {df['영업일자'].min().date()} ~ {df['영업일자'].max().date()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ASR1qEDocjX",
        "outputId": "4ad79e57-2450-41be-9a68-81b57bbcc9bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature engineering...\n",
            "Feature engineering complete\n",
            "▶ weight 분포\n",
            "weight\n",
            "0.2500    20530\n",
            "0.3625     9498\n",
            "0.7500    12065\n",
            "1.0000    19910\n",
            "1.0875     5380\n",
            "1.4500    10542\n",
            "3.0000    12536\n",
            "4.3500     6811\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\syyun\\AppData\\Local\\Temp\\ipykernel_14388\\3274158213.py:71: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train['weight'] = 1.0\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# 3) 피처 엔지니어링\n",
        "# -----------------------------\n",
        "print(\"Feature engineering...\")\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "train[\"item_id\"] = encoder.fit_transform(train[\"영업장명_메뉴명\"])\n",
        "\n",
        "# 영업일자에서 날짜 관련 피처 생성\n",
        "def make_date_feats(df):\n",
        "    out = df.copy()\n",
        "    out[\"year\"], out[\"month\"], out[\"day\"], out[\"weekday\"] = (\n",
        "        out[\"영업일자\"].dt.year,\n",
        "        out[\"영업일자\"].dt.month,\n",
        "        out[\"영업일자\"].dt.day,\n",
        "        out[\"영업일자\"].dt.weekday,\n",
        "    )\n",
        "    out[\"is_weekend\"] = (\n",
        "        out[\"weekday\"].isin([5, 6]).astype(int)\n",
        "    )\n",
        "\n",
        "    # 주기적 특성 변환\n",
        "    out[\"month_sin\"], out[\"month_cos\"] = np.sin(\n",
        "        2 * np.pi * out[\"month\"] / 12.0\n",
        "    ), np.cos(2 * np.pi * out[\"month\"] / 12.0)\n",
        "    out[\"wday_sin\"], out[\"wday_cos\"] = np.sin(2 * np.pi * out[\"weekday\"] / 7.0), np.cos(\n",
        "        2 * np.pi * out[\"weekday\"] / 7.0\n",
        "    )\n",
        "    return out\n",
        "\n",
        "# 영업일자 피처 생성 및 정렬\n",
        "train = make_date_feats(train)\n",
        "train = train.sort_values([\"item_id\", \"영업일자\"])\n",
        "\n",
        "for lag in [1, 7, 14, 28]:\n",
        "    train[f\"lag_{lag}\"] = train.groupby(\"item_id\")[\"매출수량\"].shift(lag)\n",
        "\n",
        "g = train.groupby(\"item_id\")[\"매출수량\"]\n",
        "train[\"roll_mean_7\"] = g.shift(1).rolling(7).mean()\n",
        "train[\"roll_mean_14\"] = g.shift(1).rolling(14).mean()\n",
        "train[\"roll_std_7\"] = g.shift(1).rolling(7).std()\n",
        "\n",
        "train = train.dropna()\n",
        "print(\"Feature engineering complete\")\n",
        "\n",
        "feature_cols = [\n",
        "    \"year\",\n",
        "    \"month\",\n",
        "    \"day\",\n",
        "    \"weekday\",\n",
        "    \"is_weekend\",\n",
        "    \"month_sin\",\n",
        "    \"month_cos\",\n",
        "    \"wday_sin\",\n",
        "    \"wday_cos\",\n",
        "    \"item_id\",\n",
        "    \"lag_1\",\n",
        "    \"lag_7\",\n",
        "    \"lag_14\",\n",
        "    \"lag_28\",\n",
        "    \"roll_mean_7\",\n",
        "    \"roll_mean_14\",\n",
        "    \"roll_std_7\",\n",
        "]\n",
        "\n",
        "# =========================================\n",
        "# [패치] 2024 가중치 + 특정 매장 가중치 + 인덱스 정렬\n",
        "# =========================================\n",
        "\n",
        "# 1) 기본 가중치\n",
        "train['weight'] = 1.0\n",
        "\n",
        "# 2) 특정 매장 가중치 (담하/미라시아) — 필요 시 값 조절\n",
        "train.loc[train['영업장명_메뉴명'].str.contains('담하|미라시아', na=False), 'weight'] *= 3\n",
        "\n",
        "# 3) 2024년 가중치\n",
        "USE_RAMP_2024 = False  # True: 2024 안에서 연초→연말 선형증가(정교), False: 고정배수(간단)\n",
        "\n",
        "mask_2024 = train['year'] == 2024\n",
        "\n",
        "# Anchor\n",
        "# 고정 배수(간단): 2024 전체 ×1.5 (원하면 1.2~2.0 사이로 조절)\n",
        "train.loc[mask_2024, 'weight'] *= 1.45\n",
        "\n",
        "# 4) 값이 0인 경우 가중치 감소\n",
        "train.loc[train['매출수량'] == 0, 'weight'] *= 0.25\n",
        "\n",
        "# (선택) 가중치 분포 확인\n",
        "print(\"▶ weight 분포\")\n",
        "print(train['weight'].value_counts().sort_index())\n",
        "\n",
        "# 5) 최종 학습 입력 구성 + 인덱스 정렬(중요)\n",
        "X = train[feature_cols].reset_index(drop=True)\n",
        "y = train[\"매출수량\"].astype(float).reset_index(drop=True)\n",
        "weights = train[\"weight\"].astype(float).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv9TZPrsocjY",
        "outputId": "f2a4f94f-eb23-4a21-d988-ce19ef340886"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-24 21:46:20,748] A new study created in memory with name: no-name-b705d21c-2727-4d9a-af8a-62f56697ce4f\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna hyperparameter tuning...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-24 21:46:23,417] Trial 0 finished with value: 25.85267337227491 and parameters: {'max_depth': 7, 'learning_rate': 0.2536999076681772, 'subsample': 0.892797576724562, 'colsample_bytree': 0.8394633936788146, 'gamma': 1.77071686435378e-07, 'lambda': 1.7699302940633311e-07, 'alpha': 2.9152036385288193e-08}. Best is trial 0 with value: 25.85267337227491.\n",
            "[I 2025-08-24 21:46:33,106] Trial 1 finished with value: 27.012028527948377 and parameters: {'max_depth': 11, 'learning_rate': 0.07725378389307355, 'subsample': 0.8832290311184181, 'colsample_bytree': 0.608233797718321, 'gamma': 0.574485163632042, 'lambda': 0.04566054873446119, 'alpha': 4.997040685255803e-07}. Best is trial 0 with value: 25.85267337227491.\n",
            "[I 2025-08-24 21:46:39,107] Trial 2 finished with value: 24.715334648590826 and parameters: {'max_depth': 5, 'learning_rate': 0.018659959624904916, 'subsample': 0.7216968971838151, 'colsample_bytree': 0.8099025726528951, 'gamma': 2.85469785779718e-05, 'lambda': 2.1371407316372935e-06, 'alpha': 0.000784915956255507}. Best is trial 2 with value: 24.715334648590826.\n",
            "[I 2025-08-24 21:46:43,823] Trial 3 finished with value: 24.639939224861006 and parameters: {'max_depth': 5, 'learning_rate': 0.027010527749605478, 'subsample': 0.7465447373174767, 'colsample_bytree': 0.7824279936868144, 'gamma': 0.019116469627784252, 'lambda': 3.9572205641009174e-07, 'alpha': 0.00012997969313168238}. Best is trial 3 with value: 24.639939224861006.\n",
            "[I 2025-08-24 21:47:04,888] Trial 4 finished with value: 26.31389413992715 and parameters: {'max_depth': 9, 'learning_rate': 0.011711509955524094, 'subsample': 0.8430179407605753, 'colsample_bytree': 0.6682096494749166, 'gamma': 3.3144597077512234e-08, 'lambda': 0.39001768308022033, 'alpha': 0.530953226900921}. Best is trial 3 with value: 24.639939224861006.\n",
            "[I 2025-08-24 21:47:05,038] Trial 5 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:05,594] Trial 6 pruned. Trial was pruned at iteration 70.\n",
            "[I 2025-08-24 21:47:06,216] Trial 7 pruned. Trial was pruned at iteration 67.\n",
            "[I 2025-08-24 21:47:06,280] Trial 8 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:06,352] Trial 9 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:08,176] Trial 10 pruned. Trial was pruned at iteration 88.\n",
            "[I 2025-08-24 21:47:12,988] Trial 11 finished with value: 24.609195263205045 and parameters: {'max_depth': 6, 'learning_rate': 0.04016074146262434, 'subsample': 0.7284214009233713, 'colsample_bytree': 0.7796337910371378, 'gamma': 0.0008702334725866433, 'lambda': 3.652124700479742e-06, 'alpha': 0.002825334642004338}. Best is trial 11 with value: 24.609195263205045.\n",
            "[I 2025-08-24 21:47:13,475] Trial 12 pruned. Trial was pruned at iteration 91.\n",
            "[I 2025-08-24 21:47:14,286] Trial 13 pruned. Trial was pruned at iteration 76.\n",
            "[I 2025-08-24 21:47:20,748] Trial 14 finished with value: 25.386489270612635 and parameters: {'max_depth': 8, 'learning_rate': 0.04107115421385951, 'subsample': 0.7006820900047528, 'colsample_bytree': 0.7790826462638242, 'gamma': 0.0004632519240035938, 'lambda': 0.0020288569587832857, 'alpha': 0.004542681145568804}. Best is trial 11 with value: 24.609195263205045.\n",
            "[I 2025-08-24 21:47:20,847] Trial 15 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:21,106] Trial 16 pruned. Trial was pruned at iteration 57.\n",
            "[I 2025-08-24 21:47:21,218] Trial 17 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:21,315] Trial 18 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:21,811] Trial 19 pruned. Trial was pruned at iteration 82.\n",
            "[I 2025-08-24 21:47:22,660] Trial 20 pruned. Trial was pruned at iteration 32.\n",
            "[I 2025-08-24 21:47:22,767] Trial 21 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:22,873] Trial 22 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:22,978] Trial 23 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:23,065] Trial 24 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:23,159] Trial 25 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:23,263] Trial 26 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:23,367] Trial 27 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:26,940] Trial 28 finished with value: 24.90124277877818 and parameters: {'max_depth': 5, 'learning_rate': 0.049874613572465955, 'subsample': 0.7901229805149966, 'colsample_bytree': 0.94388089318239, 'gamma': 0.010839819140938216, 'lambda': 0.0002354686191158777, 'alpha': 0.0001930615503056095}. Best is trial 11 with value: 24.609195263205045.\n",
            "[I 2025-08-24 21:47:27,087] Trial 29 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:27,225] Trial 30 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:27,606] Trial 31 pruned. Trial was pruned at iteration 70.\n",
            "[I 2025-08-24 21:47:27,969] Trial 32 pruned. Trial was pruned at iteration 83.\n",
            "[I 2025-08-24 21:47:28,071] Trial 33 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:28,317] Trial 34 pruned. Trial was pruned at iteration 47.\n",
            "[I 2025-08-24 21:47:28,409] Trial 35 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:28,760] Trial 36 pruned. Trial was pruned at iteration 25.\n",
            "[I 2025-08-24 21:47:32,925] Trial 37 finished with value: 25.07288755182973 and parameters: {'max_depth': 5, 'learning_rate': 0.04972754425800694, 'subsample': 0.6433851356207372, 'colsample_bytree': 0.8710081026547697, 'gamma': 0.006268702214530644, 'lambda': 0.968877095795058, 'alpha': 0.11620020867904036}. Best is trial 11 with value: 24.609195263205045.\n",
            "[I 2025-08-24 21:47:33,043] Trial 38 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:36,137] Trial 39 finished with value: 25.003352901698754 and parameters: {'max_depth': 6, 'learning_rate': 0.07326703362074531, 'subsample': 0.6794321568295104, 'colsample_bytree': 0.9698494938430192, 'gamma': 0.00016179397631975253, 'lambda': 1.520860891474242e-05, 'alpha': 0.0005478579783930952}. Best is trial 11 with value: 24.609195263205045.\n",
            "[I 2025-08-24 21:47:36,228] Trial 40 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:47:39,519] Trial 41 finished with value: 24.25419583894149 and parameters: {'max_depth': 6, 'learning_rate': 0.07577909344129312, 'subsample': 0.686642243839317, 'colsample_bytree': 0.9675643393766034, 'gamma': 9.684413794535215e-06, 'lambda': 1.0814961498950764e-05, 'alpha': 0.0004974372312199882}. Best is trial 41 with value: 24.25419583894149.\n",
            "[I 2025-08-24 21:47:42,751] Trial 42 finished with value: 24.388164209207996 and parameters: {'max_depth': 5, 'learning_rate': 0.09130709673260953, 'subsample': 0.7581082506186003, 'colsample_bytree': 0.9044967447434752, 'gamma': 1.3826075165186644e-05, 'lambda': 6.273616313611831e-06, 'alpha': 0.002253733695069459}. Best is trial 41 with value: 24.25419583894149.\n",
            "[I 2025-08-24 21:47:43,053] Trial 43 pruned. Trial was pruned at iteration 31.\n",
            "[I 2025-08-24 21:47:43,269] Trial 44 pruned. Trial was pruned at iteration 22.\n",
            "[I 2025-08-24 21:47:46,653] Trial 45 finished with value: 24.615617166154372 and parameters: {'max_depth': 6, 'learning_rate': 0.0755287633338884, 'subsample': 0.7169058760182709, 'colsample_bytree': 0.9241250369981516, 'gamma': 2.026730748652848e-05, 'lambda': 8.017081686138139e-06, 'alpha': 0.005834102914488458}. Best is trial 41 with value: 24.25419583894149.\n",
            "[I 2025-08-24 21:47:51,186] Trial 46 finished with value: 24.739025247240182 and parameters: {'max_depth': 7, 'learning_rate': 0.0858717803996003, 'subsample': 0.7123924575939459, 'colsample_bytree': 0.953049091965845, 'gamma': 1.753157019231062e-05, 'lambda': 7.683094321506701e-06, 'alpha': 0.037812776105567304}. Best is trial 41 with value: 24.25419583894149.\n",
            "[I 2025-08-24 21:47:55,376] Trial 47 finished with value: 24.710836462054306 and parameters: {'max_depth': 6, 'learning_rate': 0.06235009164082185, 'subsample': 0.745560010547776, 'colsample_bytree': 0.9130877062894194, 'gamma': 4.882043352017781e-06, 'lambda': 3.338332774599181e-05, 'alpha': 0.004837353637295861}. Best is trial 41 with value: 24.25419583894149.\n",
            "[I 2025-08-24 21:47:55,676] Trial 48 pruned. Trial was pruned at iteration 23.\n",
            "[I 2025-08-24 21:47:55,953] Trial 49 pruned. Trial was pruned at iteration 30.\n",
            "[I 2025-08-24 21:47:56,220] Trial 50 pruned. Trial was pruned at iteration 25.\n",
            "[I 2025-08-24 21:47:56,332] Trial 51 pruned. Trial was pruned at iteration 6.\n",
            "[I 2025-08-24 21:47:58,955] Trial 52 finished with value: 23.84409384008459 and parameters: {'max_depth': 5, 'learning_rate': 0.11445424740515175, 'subsample': 0.7523840346576761, 'colsample_bytree': 0.8844934445010932, 'gamma': 2.063615382380159e-05, 'lambda': 0.00010747087773482173, 'alpha': 0.025807405198685764}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:01,573] Trial 53 finished with value: 24.125757552138882 and parameters: {'max_depth': 5, 'learning_rate': 0.1215695775189909, 'subsample': 0.7696536112236393, 'colsample_bytree': 0.8879638863929333, 'gamma': 2.3406139656819752e-05, 'lambda': 0.0001304978654970583, 'alpha': 0.02855543561002125}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:04,164] Trial 54 finished with value: 24.915272280413774 and parameters: {'max_depth': 5, 'learning_rate': 0.11948201054873861, 'subsample': 0.7673515236380307, 'colsample_bytree': 0.8875757969767609, 'gamma': 2.025627383201239e-05, 'lambda': 0.00010646285206757389, 'alpha': 0.07938921121968663}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:04,303] Trial 55 pruned. Trial was pruned at iteration 18.\n",
            "[I 2025-08-24 21:48:04,450] Trial 56 pruned. Trial was pruned at iteration 13.\n",
            "[I 2025-08-24 21:48:04,603] Trial 57 pruned. Trial was pruned at iteration 21.\n",
            "[I 2025-08-24 21:48:07,641] Trial 58 finished with value: 24.172353926750827 and parameters: {'max_depth': 6, 'learning_rate': 0.10869771719254873, 'subsample': 0.7822481988965694, 'colsample_bytree': 0.8985272678643322, 'gamma': 0.0002379468358437256, 'lambda': 2.1584713404759213e-05, 'alpha': 0.0011437127070513392}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:10,071] Trial 59 finished with value: 24.510029664655995 and parameters: {'max_depth': 4, 'learning_rate': 0.11013199294688569, 'subsample': 0.7776967544625264, 'colsample_bytree': 0.8304696510307279, 'gamma': 0.00021382423419036653, 'lambda': 0.00015274955494996013, 'alpha': 0.001162493855552669}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:12,507] Trial 60 finished with value: 24.636878405920093 and parameters: {'max_depth': 4, 'learning_rate': 0.11040410166003807, 'subsample': 0.7763886975380164, 'colsample_bytree': 0.8965046605323359, 'gamma': 0.00019873015581073285, 'lambda': 0.0005710302012693697, 'alpha': 0.001212930073166821}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:12,681] Trial 61 pruned. Trial was pruned at iteration 18.\n",
            "[I 2025-08-24 21:48:12,847] Trial 62 pruned. Trial was pruned at iteration 14.\n",
            "[I 2025-08-24 21:48:13,027] Trial 63 pruned. Trial was pruned at iteration 21.\n",
            "[I 2025-08-24 21:48:15,656] Trial 64 finished with value: 25.008868775973564 and parameters: {'max_depth': 4, 'learning_rate': 0.08735594562085493, 'subsample': 0.7348170862354149, 'colsample_bytree': 0.816036558434919, 'gamma': 0.00036124605090357705, 'lambda': 0.0005721617841791435, 'alpha': 0.0009730376050112504}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:18,466] Trial 65 finished with value: 24.70214178012929 and parameters: {'max_depth': 5, 'learning_rate': 0.09948007814291292, 'subsample': 0.8435785188397444, 'colsample_bytree': 0.9102044131173498, 'gamma': 0.00013118370750494002, 'lambda': 7.505323397385565e-05, 'alpha': 0.44978978958657545}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:21,439] Trial 66 finished with value: 24.657419076136996 and parameters: {'max_depth': 6, 'learning_rate': 0.1187996981289723, 'subsample': 0.791850426534323, 'colsample_bytree': 0.8852097276663576, 'gamma': 8.629697355554638e-06, 'lambda': 2.2309831935197963e-05, 'alpha': 0.00040022739548652977}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:21,703] Trial 67 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:21,869] Trial 68 pruned. Trial was pruned at iteration 15.\n",
            "[I 2025-08-24 21:48:21,997] Trial 69 pruned. Trial was pruned at iteration 16.\n",
            "[I 2025-08-24 21:48:25,427] Trial 70 finished with value: 24.76134477472501 and parameters: {'max_depth': 5, 'learning_rate': 0.0835678058989514, 'subsample': 0.8029914429873386, 'colsample_bytree': 0.9358311122312802, 'gamma': 0.00024646025858481165, 'lambda': 0.0003903104892461874, 'alpha': 0.0008155841630575206}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:25,534] Trial 71 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:25,689] Trial 72 pruned. Trial was pruned at iteration 15.\n",
            "[I 2025-08-24 21:48:25,793] Trial 73 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:25,898] Trial 74 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:26,006] Trial 75 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:26,117] Trial 76 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:26,257] Trial 77 pruned. Trial was pruned at iteration 15.\n",
            "[I 2025-08-24 21:48:26,517] Trial 78 pruned. Trial was pruned at iteration 7.\n",
            "[I 2025-08-24 21:48:26,621] Trial 79 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:26,775] Trial 80 pruned. Trial was pruned at iteration 14.\n",
            "[I 2025-08-24 21:48:26,931] Trial 81 pruned. Trial was pruned at iteration 19.\n",
            "[I 2025-08-24 21:48:27,088] Trial 82 pruned. Trial was pruned at iteration 19.\n",
            "[I 2025-08-24 21:48:27,242] Trial 83 pruned. Trial was pruned at iteration 18.\n",
            "[I 2025-08-24 21:48:27,393] Trial 84 pruned. Trial was pruned at iteration 16.\n",
            "[I 2025-08-24 21:48:27,553] Trial 85 pruned. Trial was pruned at iteration 17.\n",
            "[I 2025-08-24 21:48:30,614] Trial 86 finished with value: 24.303200867647053 and parameters: {'max_depth': 6, 'learning_rate': 0.10727467968992785, 'subsample': 0.7512170134907998, 'colsample_bytree': 0.9794889857347197, 'gamma': 7.940562382388647e-06, 'lambda': 0.004294137412087521, 'alpha': 0.01599577464559136}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:30,819] Trial 87 pruned. Trial was pruned at iteration 19.\n",
            "[I 2025-08-24 21:48:30,934] Trial 88 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:31,046] Trial 89 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:31,238] Trial 90 pruned. Trial was pruned at iteration 20.\n",
            "[I 2025-08-24 21:48:34,260] Trial 91 finished with value: 24.675720189170875 and parameters: {'max_depth': 6, 'learning_rate': 0.10600307020323847, 'subsample': 0.7921818223936975, 'colsample_bytree': 0.9147290754363531, 'gamma': 0.00011555896697984435, 'lambda': 0.000930751322594313, 'alpha': 0.0014584147467657205}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:34,410] Trial 92 pruned. Trial was pruned at iteration 18.\n",
            "[I 2025-08-24 21:48:34,562] Trial 93 pruned. Trial was pruned at iteration 15.\n",
            "[I 2025-08-24 21:48:34,695] Trial 94 pruned. Trial was pruned at iteration 11.\n",
            "[I 2025-08-24 21:48:37,938] Trial 95 finished with value: 24.303481542985413 and parameters: {'max_depth': 6, 'learning_rate': 0.10069707892104644, 'subsample': 0.7488563978676036, 'colsample_bytree': 0.8918748202176163, 'gamma': 0.0013006254907696995, 'lambda': 0.002169327721014101, 'alpha': 0.06067490675513938}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:38,068] Trial 96 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:38,247] Trial 97 pruned. Trial was pruned at iteration 13.\n",
            "[I 2025-08-24 21:48:38,382] Trial 98 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:38,503] Trial 99 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:38,621] Trial 100 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:38,905] Trial 101 pruned. Trial was pruned at iteration 17.\n",
            "[I 2025-08-24 21:48:41,601] Trial 102 finished with value: 24.64692758879345 and parameters: {'max_depth': 5, 'learning_rate': 0.11585871025241833, 'subsample': 0.7615623790538762, 'colsample_bytree': 0.9076159774300273, 'gamma': 0.0012946224171067628, 'lambda': 0.0018809124050001686, 'alpha': 0.0005883285427031938}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:44,213] Trial 103 finished with value: 24.514539172099926 and parameters: {'max_depth': 4, 'learning_rate': 0.12399669510490036, 'subsample': 0.7209566731837147, 'colsample_bytree': 0.9240633515540688, 'gamma': 0.00029017365665923563, 'lambda': 0.00015021576443216027, 'alpha': 0.001003583913242461}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:44,361] Trial 104 pruned. Trial was pruned at iteration 14.\n",
            "[I 2025-08-24 21:48:46,884] Trial 105 finished with value: 24.75744223523518 and parameters: {'max_depth': 6, 'learning_rate': 0.12403070930313735, 'subsample': 0.7099756664713843, 'colsample_bytree': 0.942530840611693, 'gamma': 0.00036807129770980536, 'lambda': 3.9647428717977346e-05, 'alpha': 0.0009653680406637313}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:46,987] Trial 106 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:47,096] Trial 107 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:47,268] Trial 108 pruned. Trial was pruned at iteration 14.\n",
            "[I 2025-08-24 21:48:47,424] Trial 109 pruned. Trial was pruned at iteration 14.\n",
            "[I 2025-08-24 21:48:47,558] Trial 110 pruned. Trial was pruned at iteration 14.\n",
            "[I 2025-08-24 21:48:50,144] Trial 111 finished with value: 24.94550587972653 and parameters: {'max_depth': 4, 'learning_rate': 0.13054067371499886, 'subsample': 0.775049615690245, 'colsample_bytree': 0.8977826098865466, 'gamma': 0.00019114077846204834, 'lambda': 0.0004142632438258034, 'alpha': 0.0003758804669563813}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:50,302] Trial 112 pruned. Trial was pruned at iteration 20.\n",
            "[I 2025-08-24 21:48:50,411] Trial 113 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:52,895] Trial 114 finished with value: 24.82230335485348 and parameters: {'max_depth': 4, 'learning_rate': 0.11987628720546328, 'subsample': 0.7692301020269047, 'colsample_bytree': 0.8812527114658644, 'gamma': 1.7176277489011786e-05, 'lambda': 0.0015542284325311064, 'alpha': 0.9461458119850769}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:53,003] Trial 115 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:53,115] Trial 116 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:56,002] Trial 117 finished with value: 24.53188061817063 and parameters: {'max_depth': 6, 'learning_rate': 0.10593262724759474, 'subsample': 0.7474319828824476, 'colsample_bytree': 0.9024226838512354, 'gamma': 0.0008136309239913429, 'lambda': 0.0008259348430331071, 'alpha': 0.00350295906730059}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:56,135] Trial 118 pruned. Trial was pruned at iteration 9.\n",
            "[I 2025-08-24 21:48:56,253] Trial 119 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:59,473] Trial 120 finished with value: 24.30526614695542 and parameters: {'max_depth': 6, 'learning_rate': 0.10462706860954893, 'subsample': 0.7621399071668918, 'colsample_bytree': 0.9522962736790389, 'gamma': 0.0011130556025564064, 'lambda': 4.770598361222924e-06, 'alpha': 0.0007032300412871966}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:48:59,642] Trial 121 pruned. Trial was pruned at iteration 17.\n",
            "[I 2025-08-24 21:48:59,756] Trial 122 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:48:59,917] Trial 123 pruned. Trial was pruned at iteration 12.\n",
            "[I 2025-08-24 21:49:00,085] Trial 124 pruned. Trial was pruned at iteration 12.\n",
            "[I 2025-08-24 21:49:02,803] Trial 125 finished with value: 24.751017004313734 and parameters: {'max_depth': 6, 'learning_rate': 0.10754577714335743, 'subsample': 0.7251229050970254, 'colsample_bytree': 0.9048783792366641, 'gamma': 0.002219929898080447, 'lambda': 1.5412979734893634e-05, 'alpha': 0.005845680250559509}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:49:02,920] Trial 126 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:03,042] Trial 127 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:03,200] Trial 128 pruned. Trial was pruned at iteration 16.\n",
            "[I 2025-08-24 21:49:03,322] Trial 129 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:06,357] Trial 130 finished with value: 24.815140628547148 and parameters: {'max_depth': 6, 'learning_rate': 0.10031903555709387, 'subsample': 0.8090208564344005, 'colsample_bytree': 0.9561458849669329, 'gamma': 2.5561258313741028e-05, 'lambda': 0.00024084995011201425, 'alpha': 0.012173535347171888}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:49:08,906] Trial 131 finished with value: 24.23821905953031 and parameters: {'max_depth': 4, 'learning_rate': 0.11153385161748053, 'subsample': 0.77846478479918, 'colsample_bytree': 0.8872657458497555, 'gamma': 0.00017012455893038555, 'lambda': 0.0005509565962891509, 'alpha': 0.001271983524546782}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:49:09,059] Trial 132 pruned. Trial was pruned at iteration 11.\n",
            "[I 2025-08-24 21:49:09,215] Trial 133 pruned. Trial was pruned at iteration 15.\n",
            "[I 2025-08-24 21:49:12,049] Trial 134 finished with value: 24.990521152106 and parameters: {'max_depth': 6, 'learning_rate': 0.1276088815038014, 'subsample': 0.7775971558608797, 'colsample_bytree': 0.9065594011372581, 'gamma': 0.0006755308988167901, 'lambda': 0.0001374441288124825, 'alpha': 0.002834851938954827}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:49:12,163] Trial 135 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:12,279] Trial 136 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:12,392] Trial 137 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:12,519] Trial 138 pruned. Trial was pruned at iteration 11.\n",
            "[I 2025-08-24 21:49:12,631] Trial 139 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:12,786] Trial 140 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:12,901] Trial 141 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:13,026] Trial 142 pruned. Trial was pruned at iteration 10.\n",
            "[I 2025-08-24 21:49:13,132] Trial 143 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:13,340] Trial 144 pruned. Trial was pruned at iteration 15.\n",
            "[I 2025-08-24 21:49:13,468] Trial 145 pruned. Trial was pruned at iteration 11.\n",
            "[I 2025-08-24 21:49:13,574] Trial 146 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:13,795] Trial 147 pruned. Trial was pruned at iteration 5.\n",
            "[I 2025-08-24 21:49:16,331] Trial 148 finished with value: 24.573341738705686 and parameters: {'max_depth': 5, 'learning_rate': 0.12185559618981302, 'subsample': 0.7866626892772991, 'colsample_bytree': 0.9852619873705707, 'gamma': 0.00013766853009885673, 'lambda': 0.0005236058880152974, 'alpha': 0.0006796382928412548}. Best is trial 52 with value: 23.84409384008459.\n",
            "[I 2025-08-24 21:49:16,474] Trial 149 pruned. Trial was pruned at iteration 14.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameter: {'max_depth': 5, 'learning_rate': 0.11445424740515175, 'subsample': 0.7523840346576761, 'colsample_bytree': 0.8844934445010932, 'gamma': 2.063615382380159e-05, 'lambda': 0.00010747087773482173, 'alpha': 0.025807405198685764}\n",
            "Best RMSE: 23.84409384008459\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# 4) Optuna를 이용한 하이퍼파라미터 튜닝 (가중치/재현성 반영 버전)\n",
        "# -----------------------------\n",
        "print(\"Optuna hyperparameter tuning...\")\n",
        "\n",
        "# 재현성 강화를 위한 전역 시드 (필요 시 상단 공통 영역으로 이동해도 무방)\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "splits = list(tscv.split(X))\n",
        "\n",
        "# 검증 점수에 가중치 반영 여부 (True: 가중 RMSE, False: 비가중 RMSE)\n",
        "USE_WEIGHTED_EVAL = True\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        \"eval_metric\": \"rmse\",\n",
        "        \"tree_method\": \"hist\",\n",
        "        # GPU의 미세한 비결정성 회피가 필요하면 'cpu' 권장\n",
        "        \"device\": \"cuda\" if CUDA else \"cpu\",\n",
        "        \"seed\": 42,\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
        "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
        "    }\n",
        "\n",
        "    rmses = []\n",
        "    for tr_idx, va_idx in splits:\n",
        "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
        "        w_tr, w_va = weights.iloc[tr_idx], weights.iloc[va_idx]  # ← 2024/특정매장 가중치 반영\n",
        "\n",
        "        dtr = xgb.DMatrix(X_tr, label=y_tr, weight=w_tr)\n",
        "        if USE_WEIGHTED_EVAL:\n",
        "            dva = xgb.DMatrix(X_va, label=y_va, weight=w_va)  # 평가도 가중\n",
        "        else:\n",
        "            dva = xgb.DMatrix(X_va, label=y_va)               # 평가 비가중\n",
        "\n",
        "        cb = optuna.integration.XGBoostPruningCallback(trial, \"val-rmse\")\n",
        "        model = xgb.train(\n",
        "            params,\n",
        "            dtr,\n",
        "            num_boost_round=1000,\n",
        "            evals=[(dva, \"val\")],\n",
        "            early_stopping_rounds=50,\n",
        "            callbacks=[cb],\n",
        "            verbose_eval=False,\n",
        "        )\n",
        "        rmses.append(float(model.best_score))\n",
        "\n",
        "    return float(np.mean(rmses))\n",
        "\n",
        "\n",
        "# Optuna Sampler에 시드 고정(탐색 경로 재현성)\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "    sampler=sampler,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    category=UserWarning,\n",
        "    message=r\"The reported value is ignored because this `step` \\d+ is already reported\\.\"\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=150)\n",
        "\n",
        "print(f\"Best hyperparameter: {study.best_params}\")\n",
        "print(f\"Best RMSE: {study.best_value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995
        },
        "id": "eXjgGw4JocjY",
        "outputId": "86c02231-f88f-4540-8290-9d562a142307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensemble model training with random windows...\n",
            "--- Training model 1/10 ---\n",
            "  Window: 2023-05-11 ~ 2023-11-07 | Validation: 2023-11-07 ~ 2023-11-14\n",
            "  Best iteration: 14, Best RMSE: 17.5607\n",
            "--- Training model 2/10 ---\n",
            "  Window: 2023-10-26 ~ 2024-04-23 | Validation: 2024-04-23 ~ 2024-04-30\n",
            "  Best iteration: 36, Best RMSE: 21.8798\n",
            "--- Training model 3/10 ---\n",
            "  Window: 2023-05-15 ~ 2023-11-11 | Validation: 2023-11-11 ~ 2023-11-18\n",
            "  Best iteration: 7, Best RMSE: 15.4458\n",
            "--- Training model 4/10 ---\n",
            "  Window: 2023-04-10 ~ 2023-10-07 | Validation: 2023-10-07 ~ 2023-10-14\n",
            "  Best iteration: 33, Best RMSE: 20.2108\n",
            "--- Training model 5/10 ---\n",
            "  Window: 2023-08-05 ~ 2024-02-01 | Validation: 2024-02-01 ~ 2024-02-08\n",
            "  Best iteration: 97, Best RMSE: 17.4323\n",
            "--- Training model 6/10 ---\n",
            "  Window: 2023-02-18 ~ 2023-08-17 | Validation: 2023-08-17 ~ 2023-08-24\n",
            "  Best iteration: 15, Best RMSE: 12.3572\n",
            "--- Training model 7/10 ---\n",
            "  Window: 2023-05-11 ~ 2023-11-07 | Validation: 2023-11-07 ~ 2023-11-14\n",
            "  Best iteration: 14, Best RMSE: 17.5607\n",
            "--- Training model 8/10 ---\n",
            "  Window: 2023-05-30 ~ 2023-11-26 | Validation: 2023-11-26 ~ 2023-12-03\n",
            "  Best iteration: 6, Best RMSE: 13.4967\n",
            "--- Training model 9/10 ---\n",
            "  Window: 2023-08-31 ~ 2024-02-27 | Validation: 2024-02-27 ~ 2024-03-05\n",
            "  Best iteration: 5, Best RMSE: 19.3738\n",
            "--- Training model 10/10 ---\n",
            "  Window: 2023-04-26 ~ 2023-10-23 | Validation: 2023-10-23 ~ 2023-10-30\n",
            "  Best iteration: 56, Best RMSE: 29.4850\n",
            "\n",
            "✅ Ensemble model training complete.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 5) 랜덤 윈도우를 이용한 앙상블 모델 학습 (수정된 부분)\n",
        "# ==============================================================================\n",
        "print(\"Ensemble model training with random windows...\")\n",
        "\n",
        "# --- 하이퍼파라미터 설정 ---\n",
        "N_MODELS = 10  # 앙상블에 사용할 모델 개수\n",
        "WINDOW_SIZE_DAYS = 180  # 학습에 사용할 1개 윈도우의 기간 (일)\n",
        "VALIDATION_SIZE_DAYS = 7 # 윈도우 바로 다음 검증 기간 (일)\n",
        "# -------------------------\n",
        "\n",
        "# Optuna로 찾은 최적 파라미터 사용\n",
        "best_params = study.best_params\n",
        "best_params.update(\n",
        "    {\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        \"eval_metric\": \"rmse\",\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"device\": \"cuda\" if CUDA else \"cpu\",\n",
        "        \"seed\": 42\n",
        "    }\n",
        ")\n",
        "\n",
        "models = []\n",
        "min_date = train['영업일자'].min()\n",
        "# 학습 윈도우와 검증 윈도우를 만들 수 있는 마지막 날짜\n",
        "max_start_date = train['영업일자'].max() - pd.Timedelta(days=WINDOW_SIZE_DAYS + VALIDATION_SIZE_DAYS)\n",
        "\n",
        "for i in range(N_MODELS):\n",
        "    print(f\"--- Training model {i+1}/{N_MODELS} ---\")\n",
        "\n",
        "    # 1. 랜덤 윈도우 생성\n",
        "    # 전체 기간 내에서 랜덤한 시작일 선택\n",
        "    days_range = (max_start_date - min_date).days\n",
        "    random_days = np.random.randint(0, days_range)\n",
        "    window_start_date = min_date + pd.Timedelta(days=random_days)\n",
        "    window_end_date = window_start_date + pd.Timedelta(days=WINDOW_SIZE_DAYS)\n",
        "    validation_end_date = window_end_date + pd.Timedelta(days=VALIDATION_SIZE_DAYS)\n",
        "\n",
        "    print(f\"  Window: {window_start_date.date()} ~ {window_end_date.date()} | Validation: {window_end_date.date()} ~ {validation_end_date.date()}\")\n",
        "\n",
        "\n",
        "    # 2. 윈도우에 해당하는 데이터 추출 및 피처 엔지니어링\n",
        "    # 주의: lag, rolling 피처는 윈도우 이전 데이터까지 포함해서 계산해야 올바릅니다.\n",
        "    data_for_features = train[train['영업일자'] <= validation_end_date].copy()\n",
        "\n",
        "    # 피처 재생성\n",
        "    for lag in [1, 7, 14, 28]:\n",
        "        data_for_features[f\"lag_{lag}\"] = data_for_features.groupby(\"item_id\")[\"매출수량\"].shift(lag)\n",
        "\n",
        "    g = data_for_features.groupby(\"item_id\")[\"매출수량\"]\n",
        "    data_for_features[\"roll_mean_7\"], data_for_features[\"roll_mean_14\"], data_for_features[\"roll_std_7\"] = (\n",
        "        g.shift(1).rolling(7).mean(),\n",
        "        g.shift(1).rolling(14).mean(),\n",
        "        g.shift(1).rolling(7).std(),\n",
        "    )\n",
        "    data_for_features = data_for_features.dropna() # 피처 생성으로 인한 NA 제거\n",
        "\n",
        "    # 3. 학습/검증 데이터 분리\n",
        "    train_window_df = data_for_features[data_for_features['영업일자'].between(window_start_date, window_end_date)]\n",
        "    valid_window_df = data_for_features[data_for_features['영업일자'] > window_end_date]\n",
        "\n",
        "    # 4. DMatrix 생성 (가중치 적용)\n",
        "    X_tr = train_window_df[feature_cols]\n",
        "    y_tr = train_window_df[\"매출수량\"]\n",
        "    w_tr = train_window_df[\"weight\"] # 기존 가중치 로직은 train 데이터프레임에 이미 적용됨\n",
        "\n",
        "    X_va = valid_window_df[feature_cols]\n",
        "    y_va = valid_window_df[\"매출수량\"]\n",
        "\n",
        "    dtr = xgb.DMatrix(X_tr, label=y_tr, weight=w_tr)\n",
        "    dva = xgb.DMatrix(X_va, label=y_va)\n",
        "\n",
        "    # 5. 모델 학습\n",
        "    model = xgb.train(\n",
        "        best_params,\n",
        "        dtr,\n",
        "        num_boost_round=5000,\n",
        "        evals=[(dva, \"val\")],\n",
        "        early_stopping_rounds=100,\n",
        "        verbose_eval=False,\n",
        "    )\n",
        "\n",
        "    print(f\"  Best iteration: {model.best_iteration}, Best RMSE: {model.best_score:.4f}\")\n",
        "    models.append(model)\n",
        "\n",
        "print(\"\\n✅ Ensemble model training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbRcG4uCocjY",
        "outputId": "7bcf1cf2-ec2f-42df-aea4-2db013aac4fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Recursive prediction with ensemble model...\n",
            "[TEST_00] step 1 | date=2024-07-14 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,282.44 | pred_mean=6.645 | pred_std=13.919\n",
            "[TEST_00] step 2 | date=2024-07-15 | rows=193 | NaN(before→after)=0->0 | pred_sum=880.16 | pred_mean=4.560 | pred_std=8.353\n",
            "[TEST_00] step 3 | date=2024-07-16 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,008.70 | pred_mean=5.226 | pred_std=12.343\n",
            "[TEST_00] step 4 | date=2024-07-17 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,045.61 | pred_mean=5.418 | pred_std=12.136\n",
            "[TEST_00] step 5 | date=2024-07-18 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,224.62 | pred_mean=6.345 | pred_std=13.684\n",
            "[TEST_00] step 6 | date=2024-07-19 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,359.11 | pred_mean=7.042 | pred_std=14.845\n",
            "[TEST_00] step 7 | date=2024-07-20 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,516.67 | pred_mean=7.858 | pred_std=12.774\n",
            "[TEST_01] step 1 | date=2024-08-18 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,111.72 | pred_mean=5.760 | pred_std=12.206\n",
            "[TEST_01] step 2 | date=2024-08-19 | rows=193 | NaN(before→after)=0->0 | pred_sum=821.14 | pred_mean=4.255 | pred_std=6.569\n",
            "[TEST_01] step 3 | date=2024-08-20 | rows=193 | NaN(before→after)=0->0 | pred_sum=844.95 | pred_mean=4.378 | pred_std=7.525\n",
            "[TEST_01] step 4 | date=2024-08-21 | rows=193 | NaN(before→after)=0->0 | pred_sum=785.24 | pred_mean=4.069 | pred_std=6.309\n",
            "[TEST_01] step 5 | date=2024-08-22 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,160.24 | pred_mean=6.012 | pred_std=12.641\n",
            "[TEST_01] step 6 | date=2024-08-23 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,222.33 | pred_mean=6.333 | pred_std=10.502\n",
            "[TEST_01] step 7 | date=2024-08-24 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,330.47 | pred_mean=6.894 | pred_std=10.213\n",
            "[TEST_02] step 1 | date=2024-09-22 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,843.22 | pred_mean=9.550 | pred_std=22.325\n",
            "[TEST_02] step 2 | date=2024-09-23 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,972.95 | pred_mean=10.223 | pred_std=22.074\n",
            "[TEST_02] step 3 | date=2024-09-24 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,863.29 | pred_mean=9.654 | pred_std=19.491\n",
            "[TEST_02] step 4 | date=2024-09-25 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,652.20 | pred_mean=8.561 | pred_std=16.707\n",
            "[TEST_02] step 5 | date=2024-09-26 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,729.09 | pred_mean=8.959 | pred_std=16.240\n",
            "[TEST_02] step 6 | date=2024-09-27 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,859.09 | pred_mean=9.633 | pred_std=16.654\n",
            "[TEST_02] step 7 | date=2024-09-28 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,219.94 | pred_mean=11.502 | pred_std=16.707\n",
            "[TEST_03] step 1 | date=2024-10-27 | rows=193 | NaN(before→after)=0->0 | pred_sum=3,418.30 | pred_mean=17.711 | pred_std=51.859\n",
            "[TEST_03] step 2 | date=2024-10-28 | rows=193 | NaN(before→after)=0->0 | pred_sum=3,226.62 | pred_mean=16.718 | pred_std=41.614\n",
            "[TEST_03] step 3 | date=2024-10-29 | rows=193 | NaN(before→after)=0->0 | pred_sum=3,256.32 | pred_mean=16.872 | pred_std=43.141\n",
            "[TEST_03] step 4 | date=2024-10-30 | rows=193 | NaN(before→after)=0->0 | pred_sum=3,163.81 | pred_mean=16.393 | pred_std=42.410\n",
            "[TEST_03] step 5 | date=2024-10-31 | rows=193 | NaN(before→after)=0->0 | pred_sum=3,914.70 | pred_mean=20.283 | pred_std=47.786\n",
            "[TEST_03] step 6 | date=2024-11-01 | rows=193 | NaN(before→after)=0->0 | pred_sum=4,024.44 | pred_mean=20.852 | pred_std=49.795\n",
            "[TEST_03] step 7 | date=2024-11-02 | rows=193 | NaN(before→after)=0->0 | pred_sum=4,148.91 | pred_mean=21.497 | pred_std=55.245\n",
            "[TEST_04] step 1 | date=2024-12-01 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,066.18 | pred_mean=10.706 | pred_std=22.960\n",
            "[TEST_04] step 2 | date=2024-12-02 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,024.08 | pred_mean=10.487 | pred_std=23.822\n",
            "[TEST_04] step 3 | date=2024-12-03 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,107.77 | pred_mean=10.921 | pred_std=28.457\n",
            "[TEST_04] step 4 | date=2024-12-04 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,995.10 | pred_mean=10.337 | pred_std=29.253\n",
            "[TEST_04] step 5 | date=2024-12-05 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,123.03 | pred_mean=11.000 | pred_std=30.194\n",
            "[TEST_04] step 6 | date=2024-12-06 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,449.13 | pred_mean=12.690 | pred_std=34.806\n",
            "[TEST_04] step 7 | date=2024-12-07 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,524.16 | pred_mean=13.079 | pred_std=31.950\n",
            "[TEST_05] step 1 | date=2025-01-05 | rows=193 | NaN(before→after)=0->0 | pred_sum=3,360.79 | pred_mean=17.413 | pred_std=48.031\n",
            "[TEST_05] step 2 | date=2025-01-06 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,532.37 | pred_mean=13.121 | pred_std=32.342\n",
            "[TEST_05] step 3 | date=2025-01-07 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,460.70 | pred_mean=12.750 | pred_std=34.491\n",
            "[TEST_05] step 4 | date=2025-01-08 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,490.64 | pred_mean=12.905 | pred_std=34.673\n",
            "[TEST_05] step 5 | date=2025-01-09 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,594.65 | pred_mean=13.444 | pred_std=33.274\n",
            "[TEST_05] step 6 | date=2025-01-10 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,989.60 | pred_mean=15.490 | pred_std=40.490\n",
            "[TEST_05] step 7 | date=2025-01-11 | rows=193 | NaN(before→after)=0->0 | pred_sum=4,059.24 | pred_mean=21.032 | pred_std=55.568\n",
            "[TEST_06] step 1 | date=2025-02-09 | rows=193 | NaN(before→after)=0->0 | pred_sum=3,978.14 | pred_mean=20.612 | pred_std=54.661\n",
            "[TEST_06] step 2 | date=2025-02-10 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,576.34 | pred_mean=13.349 | pred_std=29.645\n",
            "[TEST_06] step 3 | date=2025-02-11 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,739.48 | pred_mean=14.194 | pred_std=32.867\n",
            "[TEST_06] step 4 | date=2025-02-12 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,367.42 | pred_mean=12.266 | pred_std=27.443\n",
            "[TEST_06] step 5 | date=2025-02-13 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,836.19 | pred_mean=14.695 | pred_std=34.186\n",
            "[TEST_06] step 6 | date=2025-02-14 | rows=193 | NaN(before→after)=0->0 | pred_sum=3,187.50 | pred_mean=16.516 | pred_std=41.089\n",
            "[TEST_06] step 7 | date=2025-02-15 | rows=193 | NaN(before→after)=0->0 | pred_sum=4,214.20 | pred_mean=21.835 | pred_std=55.262\n",
            "[TEST_07] step 1 | date=2025-03-16 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,245.09 | pred_mean=6.451 | pred_std=11.919\n",
            "[TEST_07] step 2 | date=2025-03-17 | rows=193 | NaN(before→after)=0->0 | pred_sum=851.28 | pred_mean=4.411 | pred_std=7.347\n",
            "[TEST_07] step 3 | date=2025-03-18 | rows=193 | NaN(before→after)=0->0 | pred_sum=765.40 | pred_mean=3.966 | pred_std=6.933\n",
            "[TEST_07] step 4 | date=2025-03-19 | rows=193 | NaN(before→after)=0->0 | pred_sum=803.98 | pred_mean=4.166 | pred_std=7.214\n",
            "[TEST_07] step 5 | date=2025-03-20 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,042.71 | pred_mean=5.403 | pred_std=9.949\n",
            "[TEST_07] step 6 | date=2025-03-21 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,251.35 | pred_mean=6.484 | pred_std=11.663\n",
            "[TEST_07] step 7 | date=2025-03-22 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,422.49 | pred_mean=7.370 | pred_std=11.101\n",
            "[TEST_08] step 1 | date=2025-04-20 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,624.36 | pred_mean=8.416 | pred_std=22.117\n",
            "[TEST_08] step 2 | date=2025-04-21 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,358.35 | pred_mean=7.038 | pred_std=16.527\n",
            "[TEST_08] step 3 | date=2025-04-22 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,833.50 | pred_mean=9.500 | pred_std=26.818\n",
            "[TEST_08] step 4 | date=2025-04-23 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,017.03 | pred_mean=10.451 | pred_std=31.039\n",
            "[TEST_08] step 5 | date=2025-04-24 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,342.24 | pred_mean=12.136 | pred_std=33.679\n",
            "[TEST_08] step 6 | date=2025-04-25 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,605.01 | pred_mean=13.497 | pred_std=37.374\n",
            "[TEST_08] step 7 | date=2025-04-26 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,299.12 | pred_mean=11.913 | pred_std=30.958\n",
            "[TEST_09] step 1 | date=2025-05-25 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,654.12 | pred_mean=8.571 | pred_std=19.998\n",
            "[TEST_09] step 2 | date=2025-05-26 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,011.88 | pred_mean=5.243 | pred_std=7.847\n",
            "[TEST_09] step 3 | date=2025-05-27 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,380.31 | pred_mean=7.152 | pred_std=13.937\n",
            "[TEST_09] step 4 | date=2025-05-28 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,418.42 | pred_mean=7.349 | pred_std=13.787\n",
            "[TEST_09] step 5 | date=2025-05-29 | rows=193 | NaN(before→after)=0->0 | pred_sum=1,751.04 | pred_mean=9.073 | pred_std=18.043\n",
            "[TEST_09] step 6 | date=2025-05-30 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,023.02 | pred_mean=10.482 | pred_std=21.109\n",
            "[TEST_09] step 7 | date=2025-05-31 | rows=193 | NaN(before→after)=0->0 | pred_sum=2,090.92 | pred_mean=10.834 | pred_std=20.086\n",
            "\n",
            "Prediction complete\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 6) 앙상블 모델을 이용한 재귀 예측\n",
        "# ==============================================================================\n",
        "print(\"\\nRecursive prediction with ensemble model...\")\n",
        "\n",
        "def predict_with_ensemble(models, dmatrix):\n",
        "    \"\"\"앙상블 모델들의 예측값 평균을 반환합니다.\"\"\"\n",
        "    # 각 모델로부터 예측 수행\n",
        "    predictions = [model.predict(dmatrix) for model in models]\n",
        "    # 예측 결과들의 평균 계산\n",
        "    return np.mean(predictions, axis=0)\n",
        "\n",
        "all_preds = []\n",
        "full_history = train.copy()\n",
        "\n",
        "step_logs = []\n",
        "\n",
        "# 기존 재귀 예측 로직과 거의 동일하나, final_model.predict -> predict_with_ensemble(models, ...)로 변경\n",
        "for test_name, test_df in tests.items():\n",
        "    test_df = test_df.copy()\n",
        "    test_df[\"item_id\"] = encoder.transform(test_df[\"영업장명_메뉴명\"])\n",
        "    test_df = make_date_feats(test_df)\n",
        "\n",
        "    history = pd.concat([full_history, test_df], ignore_index=True)\n",
        "    history = history.sort_values([\"item_id\", \"영업일자\"])\n",
        "\n",
        "    last_date = test_df[\"영업일자\"].max()\n",
        "    items = test_df[\"영업장명_메뉴명\"].unique()\n",
        "\n",
        "    preds_rows = []\n",
        "    current_date = last_date\n",
        "    for step in range(1, 8):\n",
        "        target_date = current_date + pd.Timedelta(days=1)\n",
        "        frame = pd.DataFrame(\n",
        "            {\"영업일자\": np.repeat(target_date, len(items)), \"영업장명_메뉴명\": items}\n",
        "        )\n",
        "        frame[\"item_id\"] = encoder.transform(frame[\"영업장명_메뉴명\"])\n",
        "        frame = make_date_feats(frame)\n",
        "\n",
        "        temp_hist = history.copy()\n",
        "        for lag in [1, 7, 14, 28]:\n",
        "            lagged = temp_hist[[\"영업일자\", \"item_id\", \"매출수량\"]].copy()\n",
        "            lagged[\"영업일자\"] = lagged[\"영업일자\"] + pd.Timedelta(days=lag)\n",
        "            frame = frame.merge(\n",
        "                lagged.rename(columns={\"매출수량\": f\"lag_{lag}\"}),\n",
        "                on=[\"영업일자\", \"item_id\"],\n",
        "                how=\"left\",\n",
        "            )\n",
        "\n",
        "        roll_base = temp_hist.sort_values([\"item_id\", \"영업일자\"]).copy()\n",
        "        gb = roll_base.groupby(\"item_id\")[\"매출수량\"]\n",
        "        roll_base[\"roll_mean_7\"] = gb.rolling(7).mean().reset_index(0, drop=True)\n",
        "        roll_base[\"roll_mean_14\"] = gb.rolling(14).mean().reset_index(0, drop=True)\n",
        "        roll_base[\"roll_std_7\"] = gb.rolling(7).std().reset_index(0, drop=True)\n",
        "        roll_base[\"영업일자\"] = roll_base[\"영업일자\"] + pd.Timedelta(days=1)\n",
        "        frame = frame.merge(\n",
        "            roll_base[\n",
        "                [\"영업일자\", \"item_id\", \"roll_mean_7\", \"roll_mean_14\", \"roll_std_7\"]\n",
        "            ],\n",
        "            on=[\"영업일자\", \"item_id\"],\n",
        "            how=\"left\",\n",
        "        )\n",
        "\n",
        "        nan_before = int(frame[feature_cols].isna().sum().sum())\n",
        "        frame[feature_cols] = frame[feature_cols].fillna(0)\n",
        "        nan_after = int(frame[feature_cols].isna().sum().sum())\n",
        "\n",
        "        # 예측 수행 (앙상블)\n",
        "        X_pred = frame[feature_cols]\n",
        "        dpred = xgb.DMatrix(X_pred)\n",
        "        # yhat = final_model.predict(dpred) # <- 기존 코드\n",
        "        yhat = predict_with_ensemble(models, dpred) # <- 변경된 코드\n",
        "        yhat = np.clip(yhat, 0, None)\n",
        "        frame[\"pred\"] = yhat\n",
        "\n",
        "        pred_sum = float(yhat.sum())\n",
        "        pred_mean = float(yhat.mean())\n",
        "        pred_std = float(yhat.std() if len(yhat) else 0.0)\n",
        "        print(\n",
        "            f\"[{test_name}] step {step} | date={target_date.date()} | \"\n",
        "            f\"rows={len(frame):,} | NaN(before→after)={nan_before}->{nan_after} | \"\n",
        "            f\"pred_sum={pred_sum:,.2f} | pred_mean={pred_mean:.3f} | pred_std={pred_std:.3f}\"\n",
        "        )\n",
        "\n",
        "        add_hist = frame[[\"영업일자\", \"item_id\", \"영업장명_메뉴명\", \"pred\"]].rename(\n",
        "            columns={\"pred\": \"매출수량\"}\n",
        "        )\n",
        "        history = pd.concat([history, add_hist], ignore_index=True)\n",
        "\n",
        "        frame_out = frame[[\"영업일자\", \"영업장명_메뉴명\", \"pred\"]].copy()\n",
        "        frame_out[\"영업일자\"] = f\"{test_name}+{step}일\"\n",
        "        preds_rows.append(frame_out)\n",
        "\n",
        "        current_date = target_date\n",
        "\n",
        "    test_pred = pd.concat(preds_rows, ignore_index=True)\n",
        "    wide = test_pred.pivot(index=\"영업일자\", columns=\"영업장명_메뉴명\", values=\"pred\")\n",
        "    all_preds.append(wide)\n",
        "\n",
        "print(\"\\nPrediction complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFZXgt5OocjZ",
        "outputId": "0a23fd77-87ce-4e95-cbab-0360edbd40bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission file created: submission_xgboost_re.csv\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# 0) 샘플 제출 파일 로드\n",
        "# -----------------------------\n",
        "sample = pd.read_csv(\"../open/sample_submission.csv\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7) 최종 제출 파일 생성\n",
        "# -----------------------------\n",
        "submission = pd.concat(all_preds)\n",
        "submission = submission.reset_index().rename(columns={\"index\": \"영업일자\"})\n",
        "submission = submission[sample.columns]   # sample과 같은 컬럼 순서로 맞춤\n",
        "out_path = 'submission_xgboost_re.csv'\n",
        "submission.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"Submission file created: {out_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRBNvMKOvb0L",
        "outputId": "4c6aad31-4fb0-49eb-bc4f-5bb1a83571f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 평가 대상 파일(submission.csv, train.csv) 로드 완료.\n",
            "\n",
            "==================================================\n",
            "             리더보드 평가 점수 (모의 테스트)\n",
            "==================================================\n",
            "Public Score  : 32.1536\n",
            "Private Score : 31.5636\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. 설정 및 데이터 로드\n",
        "# ==============================================================================\n",
        "# [사용자 수정 필요] 파일 경로\n",
        "#SUBMISSION_PATH = 'submission_wide_format.csv'\n",
        "SUBMISSION_PATH = 'submission_xgboost_re.csv'\n",
        "TRAIN_PATH = '../open/train/train.csv'\n",
        "\n",
        "try:\n",
        "    submission_df = pd.read_csv(SUBMISSION_PATH)\n",
        "    train_df = pd.read_csv(TRAIN_PATH)\n",
        "    print(\"✅ 평가 대상 파일(submission.csv, train.csv) 로드 완료.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ 오류: 평가에 필요한 파일을 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
        "    submission_df, train_df = None, None\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. 평가 함수 정의\n",
        "# ==============================================================================\n",
        "def weighted_smape(y_true, y_pred, weights):\n",
        "    \"\"\"\n",
        "    가중치가 적용된 SMAPE를 계산하고 100점 만점으로 변환하는 함수.\n",
        "    \"\"\"\n",
        "    # 실제값이 0인 데이터 필터링\n",
        "    non_zero_mask = y_true != 0\n",
        "    y_true_f = y_true[non_zero_mask]\n",
        "    y_pred_f = y_pred[non_zero_mask]\n",
        "    weights_f = weights[non_zero_mask]\n",
        "\n",
        "    # SMAPE 계산\n",
        "    numerator = np.abs(y_pred_f - y_true_f)\n",
        "    denominator = (np.abs(y_true_f) + np.abs(y_pred_f))\n",
        "\n",
        "    # 분모가 0이 되는 경우 방지 (y_true, y_pred 모두 0)\n",
        "    # 위 필터로 y_true가 0인 경우는 제외되었으므로, y_pred만 0이어도 분모는 0이 아님\n",
        "    element_wise_smape = 2 * numerator / denominator\n",
        "\n",
        "    # 가중 평균 계산\n",
        "    weighted_smape_score = np.sum(element_wise_smape * weights_f) / np.sum(weights_f)\n",
        "\n",
        "    # 100점 만점 기준으로 변환\n",
        "    return 100 * (1 - weighted_smape_score)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. 평가 실행 블록\n",
        "# ==============================================================================\n",
        "if submission_df is not None and train_df is not None:\n",
        "\n",
        "    # --- 1. 데이터 전처리 ---\n",
        "    # submission_df를 Long 포맷으로 변환\n",
        "    pred_long = pd.melt(submission_df, id_vars=['영업일자'], var_name='영업장명_메뉴명', value_name='예측값')\n",
        "\n",
        "    # train_df의 날짜 타입 변환\n",
        "    train_df['영업일자'] = pd.to_datetime(train_df['영업일자'])\n",
        "\n",
        "    # submission의 'TEST_XX+Y일'을 train의 실제 날짜와 매칭시키기 위한 가상 날짜 생성\n",
        "    # (실제 평가에서는 이 부분이 주최측의 비공개 로직에 따라 달라짐)\n",
        "    # 여기서는 train 데이터의 마지막 날짜 이후로 가정하여 매칭\n",
        "    last_train_date = train_df['영업일자'].max()\n",
        "    test_base_dates = {f'TEST_{i:02d}': last_train_date - pd.to_timedelta((9-i)*7 + 27, unit='d') for i in range(10)}\n",
        "\n",
        "    def map_to_real_date(date_id):\n",
        "        test_num = date_id.split('+')[0]\n",
        "        day_offset = int(date_id.split('+')[1].replace('일', ''))\n",
        "        return test_base_dates[test_num] + pd.to_timedelta(day_offset, unit='d')\n",
        "\n",
        "    pred_long['실제영업일자'] = pred_long['영업일자'].apply(map_to_real_date)\n",
        "\n",
        "    # --- 2. 실제값과 예측값 병합 ---\n",
        "    eval_df = pd.merge(\n",
        "        train_df,\n",
        "        pred_long,\n",
        "        left_on=['영업장명_메뉴명', '영업일자'],\n",
        "        right_on=['영업장명_메뉴명', '실제영업일자'],\n",
        "        how='inner' # 일치하는 날짜와 메뉴만 평가\n",
        "    )\n",
        "\n",
        "    if eval_df.empty:\n",
        "        print(\"\\n 경고: 예측 기간과 일치하는 실제값 데이터가 train.csv에 없습니다. 점수를 계산할 수 없습니다.\")\n",
        "    else:\n",
        "        # --- 3. 가중치 부여 ---\n",
        "        eval_df['영업장명'] = eval_df['영업장명_메뉴명'].str.split('_').str[0]\n",
        "        # '담하', '미라시아'에 가중치 2, 나머지에 1 부여 (임의 설정)\n",
        "        eval_df['가중치'] = np.where(eval_df['영업장명'].isin(['담하', '미라시아']), 2.0, 1.0)\n",
        "\n",
        "        # --- 4. 점수 계산 ---\n",
        "        y_true = eval_df['매출수량']\n",
        "        y_pred = eval_df['예측값']\n",
        "        weights = eval_df['가중치']\n",
        "\n",
        "        # Public Score (50% 샘플링)\n",
        "        public_sample = eval_df.sample(frac=0.5, random_state=42)\n",
        "        public_score = weighted_smape(public_sample['매출수량'], public_sample['예측값'], public_sample['가중치'])\n",
        "\n",
        "        # Private Score (100% 데이터)\n",
        "        private_score = weighted_smape(y_true, y_pred, weights)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"             리더보드 평가 점수 (모의 테스트)\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Public Score  : {public_score:.4f}\")\n",
        "        print(f\"Private Score : {private_score:.4f}\")\n",
        "        print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
